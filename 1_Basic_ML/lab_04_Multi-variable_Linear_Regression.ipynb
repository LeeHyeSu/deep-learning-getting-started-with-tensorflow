{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Multi-variable Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variable linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting exam score - regression using three inputs (x1, x2, x3)\n",
    "![image](https://user-images.githubusercontent.com/37262132/109262594-2abc6b80-7845-11eb-9fd1-9edc297ee644.png)\n",
    "Test Scores for General Psychology ( https://goo.gl/g2T8Kp )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dot product(=scalar product, 내적)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/37262132/109263699-12e5e700-7847-11eb-9ea7-1a4826261842.png)\n",
    "https://www.mathsisfun.com/algebra/matrix-multiplying.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-feature regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/37262132/109263180-29d80980-7846-11eb-9287-17be790b7bd3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis using matrix¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/37262132/109263228-407e6080-7846-11eb-8961-5aef8bc14174.png)\n",
    "![image](https://user-images.githubusercontent.com/37262132/109263241-48d69b80-7846-11eb-8ad8-26ddc9d0cbb3.png)\n",
    "(W, X 는 matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis without b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/37262132/109263294-5ee45c00-7846-11eb-80bc-87b13fdce65b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many x instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/37262132/109263348-70c5ff00-7846-11eb-8ffe-57de2a491cb5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5는 데이터(instance)의 수, 3은 변수(feature)의 수, 1은 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis using matrix (n output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/37262132/109263501-b4b90400-7846-11eb-9610-b26ff78ada37.png)\n",
    "- n은 데이터(instance)의 개수, 2는 결과 값의 개수로 주어진다.\n",
    "- 이때, W [?, ?] ⇒ [3, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WX vs XW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory (Lecture) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/37262132/109263557-cf8b7880-7846-11eb-913e-0655d925249d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow (Implementation) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/37262132/109263572-d4502c80-7846-11eb-9218-d36f2df63825.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example (2 variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/37262132/109263646-fa75cc80-7846-11eb-888a-b20ae40d3e77.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 335.280823 |    -4.0663 |     1.1220 |  -6.065215\n",
      "   50 |  76.037262 |    -0.8001 |     1.6209 |  -4.978779\n",
      "  100 |  18.959265 |     0.7151 |     1.8781 |  -4.429109\n",
      "  150 |   6.310240 |     1.4125 |     2.0104 |  -4.134423\n",
      "  200 |   3.445082 |     1.7284 |     2.0768 |  -3.961648\n",
      "  250 |   2.743659 |     1.8667 |     2.1075 |  -3.847750\n",
      "  300 |   2.525401 |     1.9225 |     2.1184 |  -3.762738\n",
      "  350 |   2.417754 |     1.9402 |     2.1181 |  -3.692262\n",
      "  400 |   2.337300 |     1.9403 |     2.1114 |  -3.629400\n",
      "  450 |   2.264998 |     1.9325 |     2.1008 |  -3.570778\n",
      "  500 |   2.196329 |     1.9213 |     2.0881 |  -3.514729\n",
      "  550 |   2.130126 |     1.9085 |     2.0741 |  -3.460409\n",
      "  600 |   2.066037 |     1.8953 |     2.0595 |  -3.407385\n",
      "  650 |   2.003917 |     1.8819 |     2.0444 |  -3.355424\n",
      "  700 |   1.943679 |     1.8686 |     2.0293 |  -3.304398\n",
      "  750 |   1.885258 |     1.8555 |     2.0141 |  -3.254230\n",
      "  800 |   1.828595 |     1.8425 |     1.9990 |  -3.204873\n",
      "  850 |   1.773636 |     1.8297 |     1.9841 |  -3.156293\n",
      "  900 |   1.720330 |     1.8171 |     1.9693 |  -3.108468\n",
      "  950 |   1.668625 |     1.8048 |     1.9547 |  -3.061379\n",
      " 1000 |   1.618474 |     1.7926 |     1.9403 |  -3.015011\n"
     ]
    }
   ],
   "source": [
    "x1_data = [1, 0, 3, 0, 5]\n",
    "x2_data = [0, 2, 0, 4, 0]\n",
    "y_data = [1, 2, 3, 4, 5]\n",
    "\n",
    "W1 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "W2 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "b = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "\n",
    "learning_rate = tf.Variable(0.001)\n",
    "\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W1 * x1_data + W2 * x2_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "        \n",
    "    W1_grad, W2_grad, b_grad = tape.gradient(cost, [W1, W2, b])\n",
    "    W1.assign_sub(learning_rate * W1_grad)\n",
    "    W2.assign_sub(learning_rate * W2_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "            i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example (2 variables with Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |  24.212524 |    -0.5847 |     0.4861 |  -0.641388\n",
      "   50 |   5.439573 |     0.2771 |     0.7026 |  -0.346325\n",
      "  100 |   1.247631 |     0.6764 |     0.8270 |  -0.201596\n",
      "  150 |   0.296814 |     0.8607 |     0.9001 |  -0.129428\n",
      "  200 |   0.075051 |     0.9451 |     0.9439 |  -0.092641\n",
      "  250 |   0.020848 |     0.9834 |     0.9705 |  -0.073335\n",
      "  300 |   0.006614 |     1.0005 |     0.9870 |  -0.062807\n",
      "  350 |   0.002502 |     1.0080 |     0.9972 |  -0.056776\n",
      "  400 |   0.001177 |     1.0110 |     1.0035 |  -0.053104\n",
      "  450 |   0.000704 |     1.0122 |     1.0075 |  -0.050701\n",
      "  500 |   0.000518 |     1.0125 |     1.0100 |  -0.049002\n",
      "  550 |   0.000437 |     1.0124 |     1.0115 |  -0.047706\n",
      "  600 |   0.000397 |     1.0123 |     1.0123 |  -0.046648\n",
      "  650 |   0.000375 |     1.0121 |     1.0128 |  -0.045734\n",
      "  700 |   0.000359 |     1.0119 |     1.0131 |  -0.044913\n",
      "  750 |   0.000346 |     1.0116 |     1.0132 |  -0.044152\n",
      "  800 |   0.000335 |     1.0114 |     1.0132 |  -0.043432\n",
      "  850 |   0.000325 |     1.0113 |     1.0131 |  -0.042741\n",
      "  900 |   0.000315 |     1.0111 |     1.0130 |  -0.042073\n",
      "  950 |   0.000305 |     1.0109 |     1.0128 |  -0.041422\n",
      " 1000 |   0.000296 |     1.0107 |     1.0127 |  -0.040786\n"
     ]
    }
   ],
   "source": [
    "x_data = [\n",
    "    [1., 0., 3., 0., 5],\n",
    "    [0., 2., 0., 4., 0]\n",
    "]\n",
    "y_data = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(tf.random.uniform((1, 2), -1.0, 1.0))\n",
    "b = tf.Variable(tf.random.uniform((1,), -1.0, 1.0))\n",
    "\n",
    "learning_rate = tf.Variable(0.001)\n",
    "\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x_data) + b  # (1, 2) * (2, 5) = (1, 5)\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "        \n",
    "        W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "        W.assign_sub(learning_rate * W_grad)\n",
    "        b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis without b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |  16.573290 |    -0.3897 |     0.4246 |    -0.6859\n",
      "   50 |   5.920134 |    -0.1282 |     0.7461 |    -0.1044\n",
      "  100 |   2.262819 |     0.0180 |     0.8799 |     0.2653\n",
      "  150 |   0.905716 |     0.1012 |     0.9319 |     0.5018\n",
      "  200 |   0.374822 |     0.1491 |     0.9495 |     0.6539\n",
      "  250 |   0.160281 |     0.1767 |     0.9535 |     0.7520\n",
      "  300 |   0.071896 |     0.1923 |     0.9528 |     0.8156\n",
      "  350 |   0.035031 |     0.2008 |     0.9510 |     0.8570\n",
      "  400 |   0.019489 |     0.2049 |     0.9493 |     0.8842\n",
      "  450 |   0.012836 |     0.2063 |     0.9482 |     0.9021\n",
      "  500 |   0.009906 |     0.2061 |     0.9476 |     0.9140\n",
      "  550 |   0.008540 |     0.2048 |     0.9474 |     0.9220\n",
      "  600 |   0.007835 |     0.2029 |     0.9475 |     0.9276\n",
      "  650 |   0.007411 |     0.2006 |     0.9478 |     0.9315\n",
      "  700 |   0.007109 |     0.1980 |     0.9483 |     0.9344\n",
      "  750 |   0.006863 |     0.1954 |     0.9489 |     0.9366\n",
      "  800 |   0.006642 |     0.1926 |     0.9495 |     0.9383\n",
      "  850 |   0.006437 |     0.1898 |     0.9502 |     0.9398\n",
      "  900 |   0.006241 |     0.1870 |     0.9509 |     0.9410\n",
      "  950 |   0.006052 |     0.1843 |     0.9516 |     0.9421\n",
      " 1000 |   0.005870 |     0.1815 |     0.9523 |     0.9431\n"
     ]
    }
   ],
   "source": [
    "# 앞의 코드에서 bias(b) 를 행렬에 추가\n",
    "x_data = [\n",
    "    [1., 1., 1., 1., 1.],  # bias(b)\n",
    "    [1., 0., 3., 0., 5.],\n",
    "    [0., 2., 0., 4., 0.]\n",
    "]\n",
    "y_data = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(tf.random.uniform((1, 3), -1.0, 1.0))  # [1, 3] 으로 변경하고 b 삭제\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x_data)  # b가 없다\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "        \n",
    "    grads = tape.gradient(cost, [W])\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads,[W]))\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.4f}\".format(\n",
    "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], W.numpy()[0][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Gradient\n",
    "- tf.train.GradientDescentOptimizer(): optimizer\n",
    "- optimizer.apply_gradients(): update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | cost\n",
      "    0 |   0.656688\n",
      "   50 |   0.047971\n",
      "  100 |   0.032798\n",
      "  150 |   0.022425\n",
      "  200 |   0.015332\n",
      "  250 |   0.010483\n",
      "  300 |   0.007167\n",
      "  350 |   0.004900\n",
      "  400 |   0.003350\n",
      "  450 |   0.002291\n",
      "  500 |   0.001566\n",
      "  550 |   0.001071\n",
      "  600 |   0.000732\n",
      "  650 |   0.000501\n",
      "  700 |   0.000342\n",
      "  750 |   0.000234\n",
      "  800 |   0.000160\n",
      "  850 |   0.000109\n",
      "  900 |   0.000075\n",
      "  950 |   0.000051\n",
      " 1000 |   0.000035\n"
     ]
    }
   ],
   "source": [
    "# Multi-variable linear regression (1)\n",
    "\n",
    "X = tf.constant([[1., 2.],\n",
    "                 [3., 4.]])\n",
    "y = tf.constant([[1.5], [3.5]])\n",
    "\n",
    "W = tf.Variable(tf.random.normal((2, 1)))\n",
    "b = tf.Variable(tf.random.normal((1,)))\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "n_epoch = 1000+1\n",
    "print(\"epoch | cost\")\n",
    "for i in range(n_epoch):\n",
    "    # Use tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = tf.matmul(X, W) + b\n",
    "        cost = tf.reduce_mean(tf.square(y_pred - y))\n",
    "        \n",
    "    # calculates the gradients of the loss\n",
    "    grads = tape.gradient(cost, [W, b])\n",
    "    \n",
    "    # updates parameters (W and b)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b]))\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting exam score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regression using three inputs (x1, x2, x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/37262132/109269255-9572a480-784f-11eb-9ee8-8597fac2dc0e.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 5793889.5000\n",
      "   50 |   64291.1484\n",
      "  100 |     715.2902\n",
      "  150 |       9.8462\n",
      "  200 |       2.0152\n",
      "  250 |       1.9252\n",
      "  300 |       1.9210\n",
      "  350 |       1.9177\n",
      "  400 |       1.9145\n",
      "  450 |       1.9114\n",
      "  500 |       1.9081\n",
      "  550 |       1.9050\n",
      "  600 |       1.9018\n",
      "  650 |       1.8986\n",
      "  700 |       1.8955\n",
      "  750 |       1.8923\n",
      "  800 |       1.8892\n",
      "  850 |       1.8861\n",
      "  900 |       1.8829\n",
      "  950 |       1.8798\n",
      " 1000 |       1.8767\n"
     ]
    }
   ],
   "source": [
    "# data and label\n",
    "x1 = [ 73.,  93.,  89.,  96.,  73.]\n",
    "x2 = [ 80.,  88.,  91.,  98.,  66.]\n",
    "x3 = [ 75.,  93.,  90., 100.,  70.]\n",
    "Y  = [152., 185., 180., 196., 142.]\n",
    "\n",
    "# weights \n",
    "w1 = tf.Variable(10.)\n",
    "w2 = tf.Variable(10.)\n",
    "w3 = tf.Variable(10.)\n",
    "b  = tf.Variable(10.)\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "for i in range(1000+1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "    # calculates the gradients of the cost\n",
    "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
    "    \n",
    "    # update w1, w2, w3 and b\n",
    "    w1.assign_sub(learning_rate * w1_grad)\n",
    "    w2.assign_sub(learning_rate * w2_grad)\n",
    "    w3.assign_sub(learning_rate * w3_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variable linear regression (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- random 초기화: tf.random_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |   11325.9121\n",
      "   50 |     135.3618\n",
      "  100 |      11.1817\n",
      "  150 |       9.7940\n",
      "  200 |       9.7687\n",
      "  250 |       9.7587\n",
      "  300 |       9.7489\n",
      "  350 |       9.7389\n",
      "  400 |       9.7292\n",
      "  450 |       9.7194\n",
      "  500 |       9.7096\n",
      "  550 |       9.6999\n",
      "  600 |       9.6903\n",
      "  650 |       9.6806\n",
      "  700 |       9.6709\n",
      "  750 |       9.6612\n",
      "  800 |       9.6517\n",
      "  850 |       9.6421\n",
      "  900 |       9.6325\n",
      "  950 |       9.6229\n",
      " 1000 |       9.6134\n"
     ]
    }
   ],
   "source": [
    "# data and label\n",
    "x1 = [ 73.,  93.,  89.,  96.,  73.]\n",
    "x2 = [ 80.,  88.,  91.,  98.,  66.]\n",
    "x3 = [ 75.,  93.,  90., 100.,  70.]\n",
    "Y  = [152., 185., 180., 196., 142.]\n",
    "\n",
    "# random weights\n",
    "w1 = tf.Variable(tf.random.normal((1,)))\n",
    "w2 = tf.Variable(tf.random.normal((1,)))\n",
    "w3 = tf.Variable(tf.random.normal((1,)))\n",
    "b = tf.Variable(tf.random.normal((1,)))\n",
    "\n",
    "for i in range(1000+1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "    # calculates the gradients of the cost\n",
    "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
    "    \n",
    "    # update w1,w2,w3 and b\n",
    "    w1.assign_sub(learning_rate * w1_grad)\n",
    "    w2.assign_sub(learning_rate * w2_grad)\n",
    "    w3.assign_sub(learning_rate * w3_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "      print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variable linear regression (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Matrix 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | cost\n",
      "    0 |  3743.1343\n",
      "  100 |     3.6005\n",
      "  200 |     3.1265\n",
      "  300 |     3.1128\n",
      "  400 |     3.0994\n",
      "  500 |     3.0860\n",
      "  600 |     3.0726\n",
      "  700 |     3.0593\n",
      "  800 |     3.0460\n",
      "  900 |     3.0329\n",
      " 1000 |     3.0199\n",
      " 1100 |     3.0069\n",
      " 1200 |     2.9939\n",
      " 1300 |     2.9811\n",
      " 1400 |     2.9682\n",
      " 1500 |     2.9555\n",
      " 1600 |     2.9429\n",
      " 1700 |     2.9303\n",
      " 1800 |     2.9177\n",
      " 1900 |     2.9053\n",
      " 2000 |     2.8929\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "    # X1,   X2,   X3,   y\n",
    "    [ 73.,  80.,  75., 152. ],\n",
    "    [ 93.,  88.,  93., 185. ],\n",
    "    [ 89.,  91.,  90., 180. ],\n",
    "    [ 96.,  98., 100., 196. ],\n",
    "    [ 73.,  66.,  70., 142. ]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# slice data\n",
    "X = data[:, :-1]\n",
    "y = data[:, [-1]]\n",
    "\n",
    "W = tf.Variable(tf.random.normal((3, 1)))\n",
    "b = tf.Variable(tf.random.normal((1,)))\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "# hypothesis, prediction function\n",
    "def predict(X) :\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "print(\"epoch | cost\")\n",
    "\n",
    "n_epochs = 2000\n",
    "for i in range(n_epochs+1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = tf.reduce_mean((tf.square(predict(X) - y)))\n",
    "    \n",
    "    # calculates the gradients of the loss\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    \n",
    "    # updates parameters (W and b)\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0821941 ],\n",
       "       [ 0.9810675 ],\n",
       "       [-0.05930979]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.3517823], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       "array([[154.3891 ],\n",
       "       [182.81396],\n",
       "       [181.60631],\n",
       "       [195.45604],\n",
       "       [140.9507 ]], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[152.0, 185.0, 180.0, 196.0, 142.0]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y  # labels, 실제값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[154.3891 ],\n",
       "       [182.81396],\n",
       "       [181.60631],\n",
       "       [195.45604],\n",
       "       [140.9507 ]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X).numpy()  # prediction, 예측값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[185.41197],\n",
       "       [177.47295]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 데이터에 대한 예측\n",
    "predict([[89., 95., 92], [84., 92., 85]]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
